{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNO2DDF1FRkqlzezPohq+vd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cvillanue/mathematics_deeplearning/blob/main/Basic_Neural_Network_Calculations_Part_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Basic Neural Network Calculation Part 1\n",
        "\n",
        "### Callyn Villanueva\n",
        "Project Summary: The project begins by implementing forward propagation, which involves the flow of data through the neural network to generate predictions. It focuses on a neural network with a single hidden layer. The input data matrix consists of two samples, with each sample represented by a vector of five features.\n",
        "\n",
        "To enable the neural network to learn from the data, weights and biases are introduced. The weights represent the strength of connections between the input data and the hidden layer neurons. They are organized in a weight matrix, with each row corresponding to the weights associated with a specific neuron in the hidden layer. Biases are adjustable offsets that influence the neuron activations and are represented by a bias matrix.\n",
        "\n",
        "Forward propagation involves several mathematical operations. The input data matrix is multiplied by the weight matrix, and the biases are added. The resulting matrix captures the weighted sum of inputs for each neuron in the hidden layer. To introduce non-linearity and enable complex modeling, an activation function, such as the sigmoid function, is applied element-wise to the matrix. This step produces the activation values for each neuron, representing their response to the input data.\n",
        "\n",
        "Moving beyond forward propagation, the project also explores backward propagation, commonly known as backpropagation. This process enables the neural network to adjust its weights and biases based on the error between the predicted outputs and the true labels. By iteratively updating the weights and biases, the network can improve its predictions over time.\n",
        "\n",
        "Backward propagation involves computing the gradient of the loss function with respect to the weights and biases. This is achieved through the application of calculus and the chain rule. The gradients are then used to update the weights and biases in a direction that minimizes the loss function.\n",
        "\n",
        "To implement backward propagation, the project combines the principles of calculus, linear algebra, and programming. The gradients are calculated by propagating the error backwards through the network, adjusting the weights and biases accordingly. The project emphasizes the importance of efficient implementation techniques, such as vectorization, to optimize the computational performance."
      ],
      "metadata": {
        "id": "O2boTvqqVkux"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Input data\n",
        "input_data = np.array([[1, 2, 3, 4, 5], [6, 7, 8, 9, 10]])\n",
        "\n",
        "# Weights for the hidden layer\n",
        "weights1 = np.array([[0.2, 0.5, -0.1, 0.3, -0.2],\n",
        "                     [0.1, -0.3, 0.4, -0.5, 0.2],\n",
        "                     [-0.3, 0.6, -0.2, 0.1, -0.4],\n",
        "                     [0.5, -0.1, 0.3, -0.4, 0.2],\n",
        "                     [-0.4, 0.2, -0.5, 0.3, -0.1]])\n",
        "\n",
        "# Bias for the hidden layer\n",
        "bias1 = np.zeros((1, 5))  # you need to match the number of neurons\n",
        "\n",
        "# Forward propagation - calculation for z1_input_to_hidden\n",
        "z1_input_to_hidden = np.dot(input_data, weights1) + bias1\n",
        "\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "output = sigmoid(z1_input_to_hidden)\n",
        "# Print the result\n",
        "print(\"___________Dot Product for each Neuron:_______\")\n",
        "print(z1_input_to_hidden)\n",
        "print()\n",
        "print(\"___________The output of the neural network is: ______________\")\n",
        "print(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dqBtFrDOOU9f",
        "outputId": "ec90a33b-75eb-4583-e88b-c6a45e9d4971"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "___________Dot Product for each Neuron:_______\n",
            "[[-5.00000000e-01  2.30000000e+00 -1.20000000e+00 -5.00000000e-01\n",
            "  -7.00000000e-01]\n",
            " [-2.22044605e-16  6.80000000e+00 -1.70000000e+00 -1.50000000e+00\n",
            "  -2.20000000e+00]]\n",
            "\n",
            "___________The output of the neural network is: ______________\n",
            "[[0.37754067 0.90887704 0.23147522 0.37754067 0.33181223]\n",
            " [0.5        0.99888746 0.15446527 0.18242552 0.09975049]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Neuron 1: [-0.5, 2.3, -1.2, -0.5, -0.7]\n",
        "Neuron 2: [-2.22044605e-16, 6.8, -1.7, -1.5, -2.2]\n",
        "\n",
        "These values represent the weighted sum of inputs for each neuron before applying the activation function."
      ],
      "metadata": {
        "id": "ZP8BoLmPAKMY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To calculate the output of the first neuron in the hidden layer, we need to perform the dot product between the input data [1, 2, 3, 4, 5] and the corresponding weights [0.2, 0.1, -0.3, 0.5, -0.4], and then add the bias term 0.1.\n",
        "\n",
        "For the first input [1, 2, 3, 4, 5]:\n",
        "\n",
        "    Dot product calculation for each neuron (not inclulding the added bias):\n",
        "    Dot product calculation for neuron 1:\n",
        "    (1 * 0.2) + (2 * 0.1) + (3 * -0.3) + (4 * 0.5) + (5 * -0.4)\n",
        "    = 0.2 + 0.2 - 0.9 + 2 - 2\n",
        "    = -0.5\n",
        "    \n",
        "    Dot product calculation for neuron 2:\n",
        "    (1 * 0.5) + (2 * -0.3) + (3 * 0.6) + (4 * -0.1) + (5 * 0.2)\n",
        "    = 0.5 - 0.6 + 1.8 - 0.4 + 1.0\n",
        "    = 2.3\n",
        "    \n",
        "    Dot product calculation for neuron 3:\n",
        "    (1 * -0.1) + (2 * 0.4) + (3 * -0.2) + (4 * 0.3) + (5 * -0.5)\n",
        "    = -0.1 + 0.8 - 0.6 + 1.2 - 2.5\n",
        "    = -0.2\n",
        "    \n",
        "    Dot product calculation for neuron 4:\n",
        "    (1 * 0.3) + (2 * -0.5) + (3 * 0.1) + (4 * -0.4) + (5 * 0.3)\n",
        "    = 0.3 - 1.0 + 0.3 - 1.6 + 1.5\n",
        "    = 0.5\n",
        "    \n",
        "    Here are the sigmoid outputs for each neuron in the hidden layer:\n",
        "    Sigmoid output for neuron 1: sigmoid(-0.5) ≈ 0.37754067\n",
        "    Sigmoid output for neuron 2: sigmoid(2.3) ≈ 0.90887704\n",
        "    Sigmoid output for neuron 3: sigmoid(-0.2) ≈ 0.450166\n",
        "    Sigmoid output for neuron 4: sigmoid(0.5) ≈ 0.62245933\n",
        "    Sigmoid output for neuron 5: sigmoid(-0.3) ≈ 0.42555748\n",
        "\n",
        "For the second input [6, 7, 8, 9, 10], the calculations are performed similarly, resulting in the corresponding sigmoid outputs.\n",
        "\n",
        "So, the output you obtained of [[0.37754067 0.90887704 0.23147522 0.37754067 0.33181223]\n",
        "[0.5 0.99888746 0.15446527 0.18242552 0.09975049]] matches the sigmoid outputs for each neuron in the hidden layer based on the given input data and weights."
      ],
      "metadata": {
        "id": "2zJxjemsOf31"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Neural Weights Table\n",
        "Here's a visualization of how the weights are being calculated with each neuron in the hidden layer, based on the provided code:\n",
        "\n",
        "```\n",
        "Input        Neuron 1    Neuron 2    Neuron 3    Neuron 4    Neuron 5\n",
        "-------      ---------   ---------   ---------   ---------   ---------\n",
        "  1            0.2         0.5        -0.1         0.3         -0.2\n",
        "  2            0.1        -0.3         0.4        -0.5          0.2\n",
        "  3           -0.3         0.6        -0.2         0.1         -0.4\n",
        "  4            0.5        -0.1         0.3        -0.4          0.2\n",
        "  5           -0.4         0.2        -0.5         0.3         -0.1\n",
        "```\n",
        "\n",
        "In this visualization, each row represents an input feature, and each column represents a weight associated with a neuron in the hidden layer. The values represent the weights connecting the input features to each neuron in the hidden layer.\n",
        "\n",
        "For example, in the first row, the values represent the weights connecting the first input feature to each neuron in the hidden layer. Similarly, in the second row, the values represent the weights connecting the second input feature to each neuron, and so on.\n",
        "\n",
        "By multiplying each input feature with its corresponding weight and summing the results, you obtain the weighted inputs for each neuron in the hidden layer."
      ],
      "metadata": {
        "id": "pFWTTNF7Oiix"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You'll notice in the given code, the input data consists of two samples represented as rows in the input_data array. This can be interpreted as a batch of two samples. In neural network terminology, processing multiple samples together is often referred to as batch processing.\n",
        "\n",
        "When forward propagating, the code calculates the dot product between the input data and the weights for the hidden layer using the np.dot(input_data, weights1.T) operation. This operation performs matrix multiplication between the input data (2 samples) and the transposed weights (hidden layer weights). The resulting output will also be a matrix with dimensions (2, number of neurons in the hidden layer).\n",
        "\n",
        "Therefore, in this case, both samples are processed together in parallel, resulting in a batch of outputs."
      ],
      "metadata": {
        "id": "_rL1OQAIO2Ms"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Input Data                 Hidden Layer Neurons (Weights)\n",
        "\n",
        "   -------                   ---------   ---------   ---------   ---------   ---------\n",
        "    1  2  3  4  5                w11         w12         w13         w14         w15\n",
        "    6  7  8  9 10                w21         w22         w23         w24         w25\n",
        "\n",
        "                  Biases: b1  b2  b3  b4  b5\n",
        "\n"
      ],
      "metadata": {
        "id": "2FwQGdYDPEiA"
      }
    }
  ]
}